{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering: false\n",
    "---\n",
    "\n",
    "# 2.5. Matrices\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sections, we learned to consider the span of a set of vectors, and alluded to the fact that to solve the regression problem in higher dimensions, we'd need to be able to project a vector onto the span of a set of vectors. Matrices will allow us to achieve this goal elegantly. It will take a few sections to get there:\n",
    "\n",
    "- In Chapter 2.5, we'll define matrices, and learn to perform basic operations on them.\n",
    "- In Chapter 2.6, we'll define the rank of a matrix, and better understand the span of the columns of a matrix.\n",
    "- In Chapter 2.7, we'll understand matrix-vector multiplication as a transformation of the vector, and learn how to invert a matrix and compute its determinant.\n",
    "- In Chapter 2.8, we'll learn to project a vector onto the span of the columns of a matrix, closing the loop with Chapter 2.3.\n",
    "\n",
    "For now, let's consider the following three vectors in $\\mathbb{R}^4$:\n",
    "\n",
    "$$\\vec u_1 = \\begin{bmatrix} 3 \\\\ 2 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\vec u_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\quad \\vec u_3 = \\begin{bmatrix} 4 \\\\ 9 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "If we stack these vectors horizontally, we produce a **matrix**:\n",
    "\n",
    "$$\\begin{bmatrix} \\mid & \\mid & \\mid \\\\ \\vec u_1 & \\vec u_2 & \\vec u_3 \\\\ \\mid & \\mid & \\mid \\end{bmatrix} = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    ":::{note} Definition: Matrix\n",
    "A **matrix** is a rectangular array of numbers, organized into rows and columns.\n",
    "\n",
    "In this class, we'll typically use uppercase letters to denote matrices. For example:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ is a matrix with 4 rows and 3 columns, so we might say that $A$ is a $4 \\times 3$ matrix, or that $A \\in \\mathbb{R}^{4 \\times 3}$.\n",
    "\n",
    "It's common to use the notation $A_{ij}$ to denote the entry in the $i$-th row and $j$-th column of $A$, e.g. $A_{23} = 9$. \n",
    "\n",
    "In `numpy`, you can access the entry in the 2nd row and 3rd column of $A$ as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=A[1, 2] # Remember that Python uses 0-based indexing!\"\n",
    "  width=\"100%\"\n",
    "  height=\"175%\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, try and find the bottom-right entry of $A$, both using positive indexing and negative indexing!\n",
    "\n",
    "I've introduced matrices in a very particular way, because I'd like for you to think of them as a collection of vectors, sometimes called **column vectors**. We'll come back to this in just a moment. You can also think of a matrix as a collection of row vectors; the example $A$ defined above consists of four row vectors in $\\mathbb{R}^3$ stacked horizontally.\n",
    "\n",
    "When we introduced vectors, $n$ was typically the placeholder we'd use for the number of components of a vector. With matrices, I like using $n$ for the number of rows, and $d$ for the number of columns. This means that, in general, a matrix is of shape $n \\times d$ and is a member of the set $\\mathbb{R}^{n \\times d}$. **This makes clear that when we've collected data, we store each of our $n$ observations in a row of our matrix.** Just be aware that using $n \\times d$ to denote the shape of a matrix is a little unorthodox; most other textbooks will use $m \\times n$ to denote the shape of a matrix. Of course, this is all arbitrary.\n",
    "\n",
    "<!-- If $A$ is an arbitrary matrix with $n$ rows and $d$ columns, i.e. $A \\in \\mathbb{R}^{n \\times d}$, we can write $A$ as:\n",
    "\n",
    "$$A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1d} \\\\ a_{21} & a_{22} & \\cdots & a_{2d} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nd} \\end{bmatrix}$$\n",
    "\n",
    "We often use the notation $a_{ij}$ to denote the entry in the $i$-th row and $j$-th column of $A$. -->\n",
    "\n",
    "\n",
    "Suppose $A$ has $n$ rows and $d$ columns, i.e. $A \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "- If $n > d$, we say that $A$ is **tall**.\n",
    "- If $n < d$, we say that $A$ is **wide**.\n",
    "- If $n = d$, we say that $A$ is **square**.\n",
    "\n",
    "As we'll see in the coming sections, square matrices are the most flexible – there are several properties and operations that are only defined for them. Unfortunately, not every matrix is square. Remember, matrices are used for storing data, and the number of observations, $n$ and number of features, $d$, don't need to be the same. (In practice, we'll often have very tall matrices, i.e. $n \\gg d$, but this is not always the case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition and Scalar Multiplication\n",
    "\n",
    "Like vectors, matrices support addition and scalar multiplication out-of-the-box, and both behave as you'd expect.\n",
    "\n",
    ":::{note} Definition: Addition\n",
    "\n",
    "Suppose $A$ and $B$ are matrices with the same shape, i.e. $A, B \\in \\mathbb{R}^{n \\times d}$. Then, the **sum** of $A$ and $B$ is the matrix $C$ with entries $C_{ij} = A_{ij} + B_{ij}$ for all $i, j$.\n",
    "\n",
    "That is, the sum is performed element-wise. The sum is also commutative: $$A + B = B + A$$\n",
    ":::\n",
    "\n",
    ":::{note} Definition: Scalar Multiplication\n",
    "\n",
    "Suppose $A$ is a matrix and $c$ is a scalar. Then, the **scalar multiple** of $A$ by $c$ is the matrix $B$ with entries $B_{ij} = c A_{ij}$ for all $i, j$.\n",
    "\n",
    ":::\n",
    "\n",
    "Let's see an example. Consider the matrices $A$ and $B$ (where $A$ is the same as in the previous example):\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix}$$\n",
    "\n",
    "Then, the operation $3A - B$ is well-defined, and produces the matrix:\n",
    "\n",
    "$$3A - B = 3 \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & {\\color{#3d81f6} \\mathbf{-1}} & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & \\color{#3d81f6} \\mathbf{8} & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 8 & 1 & 9 \\\\ 2 & -2 & 21 \\\\ -7 & \\boxed{\\color{#3d81f6} \\mathbf{-11}} & -9 \\\\ -4 & -17 & -12 \\end{bmatrix}$$\n",
    "\n",
    "I've colored the entry at position $(3, 2)$ $\\color{#3d81f6} \\text{blue}$ to help you trace the computation:\n",
    "\n",
    "$$3 \\cdot (-1) - 8 = -11$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Vector Multiplication\n",
    "\n",
    "Great – we know how to add two matrices, and how to multiply a matrix by a scalar. The natural next step is to figure out how to multiply two matrices together, and to understand _why_ we might do so. (Hopefully, the \"why\" has something to do with data.)\n",
    "\n",
    "First, a definition.\n",
    "\n",
    ":::{attention} Golden Rule of Matrix Multiplication\n",
    "Suppose $A$ and $B$ are two matrices. In order for the product $AB$ to be valid, **the number of columns in $A$ must equal the number of rows in $B$**. \n",
    "\n",
    "$$\\text{\\# columns in } A = \\text{\\# rows in } B$$\n",
    "\n",
    "In other words, **the inner dimensions of $A$ and $B$ must match**.\n",
    ":::\n",
    "\n",
    "Let's use the Golden Rule. First – as the title of this subsection suggests – we'll start by computing the product of a matrix and a vector.\n",
    "\n",
    "Let's suppose $A$ is the same $4 \\times 3$ matrix we've become familiar with:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    "\n",
    "And let's suppose $\\vec v$ is some vector. Note that we can think of an $n$-dimensional vector as a matrix with $n$ rows and 1 column. In order for the product $A \\vec v$ to be valid, $\\vec v$ must have 3 elements in it, by the Golden Rule above. To make the example concrete, let's suppose:\n",
    "\n",
    "$$\\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "How do we multiply $A \\vec v$? The following key definition will help us.\n",
    "\n",
    ":::{note} Definition: Matrix-Vector Multiplication\n",
    "\n",
    "If $A$ is a $n \\times d$ matrix and $\\vec v$ is a $d \\times 1$ vector, then the product $A \\vec v$ is a $n \\times 1$ vector that contains **the dot product of each _row_ of $A$ with $\\vec v$**.\n",
    ":::\n",
    "\n",
    "$A$ has 4 rows, and $\\vec v$ has 1 column (itself), so we'll need to compute 4 dot products. More on this shortly.\n",
    "\n",
    "Before actually performing the multiplication, we should be aware of what the dimensions of the output are going to be. Below, we've written the dimensions of $A$ ($4 \\times 3$) and $\\vec v$ ($3 \\times 1$) next to each other. By the Golden Rule, the **inner dimensions, both of which are bolded**, must be equal in order for the multiplication to be valid. The dimensions of the output will be the result of looking at the $\\boxed{\\text{outer dimensions}}$, which here are $4 \\times 1$.\n",
    "\n",
    "$$\n",
    "\\underbrace{A}_{\\boxed{4} \\times \\textbf{3}} \\:\\:\\:\\: \\underbrace{\\vec v}_{\\textbf{3} \\times \\boxed{1}} = \\underbrace{\\text{output}}_{4 \\times 1}\n",
    "$$\n",
    "\n",
    "So, the result of multiplying $A \\vec v$ will be $4 \\times 1$ matrix, or in other words, a vector with 4 components. Indeed, the result of multiply a matrix by a vector always results in another vector, and this act of multiplying a matrix by a vector is often thought of as **transforming** the vector from $\\mathbb{R}^d$ to $\\mathbb{R}^n$.\n",
    "\n",
    "So, how do we find those 4 components? As mentioned earlier, we compute each component by taking the dot product of a row in $A$ with $\\vec v$.\n",
    "\n",
    "$$A = \\begin{bmatrix} \\color{#3d81f6} \\mathbf{3} & \\color{#3d81f6} \\mathbf{1} & \\color{#3d81f6} \\mathbf{4} \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\quad \\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "Let's start with the top row of $A$ (colored blue above), which is $\\begin{bmatrix} 3 \\\\ 1 \\\\ 4\\end{bmatrix}$. The dot product of two vectors is only defined if they have equal lengths. **This is why we've instituted the Golden Rule!** The Golden Rule tells us we can only multiply $A$ and $\\vec v$ if the number of columns in $A$ is the same as the number of components in $\\vec v$, which is true here (both of those numbers are 3).\n",
    "\n",
    "Then, the dot product of the first row of $A$ with $\\vec v$ is:\n",
    "\n",
    "$$\\begin{bmatrix} \\color{#3d81f6} \\mathbf{3} \\\\ \\color{#3d81f6} \\mathbf{1} \\\\ \\color{#3d81f6} \\mathbf{4} \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = {\\color{#3d81f6} \\mathbf{3}} \\cdot 1 + {\\color{#3d81f6} \\mathbf{1}} \\cdot 0 + {\\color{#3d81f6} \\mathbf{4}} \\cdot 3 = 15$$\n",
    "\n",
    "Nice! We're a quarter of the way there. Now, we just need to compute the remaining three dot products:\n",
    "\n",
    "- The dot product of the second row of $A$ with $\\vec v$ is $\\begin{bmatrix} 2 \\\\ 1 \\\\ 9 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 29$.\n",
    "\n",
    "- The dot product of the third row of $A$ with $\\vec v$ is $\\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 0$.\n",
    "\n",
    "- And finally, the dot product of the fourth row of $A$ with $\\vec v$ is $\\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 2$.\n",
    "\n",
    "The result of our matrix-vector multiplication, then, is the result of stacking all 4 dot products together into the vector $\\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2\\end{bmatrix}$. To summarize:\n",
    "\n",
    "$$A \\vec v = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Have them do a computation that unravels the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Combination Interpretation\n",
    "\n",
    "We've described matrix-vector multiplication as the result of taking the dot product of each row of $A$ with $\\vec v$, and indeed this is the easiest way to actually compute the output. But, there's another more importantinterpretation. In the above dot products, you may have noticed:\n",
    "\n",
    "- Entries in the first column of $A$ ($3$, $2$, $0$, and $2$) were always multiplied by the first element of $\\vec v$ ($1$).\n",
    "- Entries in the second column of $A$ ($1$, $1$, $-1$, and $-2$) were always multiplied by the second element of $\\vec v$ ($0$).\n",
    "- Entries in the third column of $A$ ($4$, $9$, $0$, and $0$) were always multiplied by the third element of $\\vec v$ ($3$).\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$A \\vec v = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} \\color{orange} \\mathbf{1} \\\\ \\color{orange} \\mathbf{0} \\\\ \\color{orange} \\mathbf{3} \\end{bmatrix} = \\underbrace{{\\color{orange} \\mathbf{1}} \\begin{bmatrix} 3 \\\\ 2 \\\\ 0 \\\\ 2 \\end{bmatrix} + {\\color{orange} \\mathbf{0}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -2 \\end{bmatrix} + {\\color{orange} \\mathbf{3}} \\begin{bmatrix} 4 \\\\ 9 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{\\color{orange} \\text{linear combination of columns of A}} = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "At the start of this section, we defined $A$ by stacking the vectors $\\vec u_1$, $\\vec u_2$, and $\\vec u_3$ side-by-side, and I told you to think of a matrix as a collection of column vectors. The above result is precisely why – it's because when we multiply $A$ by $\\vec v$, we're computing a linear combination of the columns of $A$, where the weights are the components of $\\vec v$!\n",
    "\n",
    "In Chapter 2.5, we will consider the span of the columns of a matrix, much as we considered the span – or set of all possible linear combinations – of a set of vectors in Chapter 2.4. This will lead us to define the **rank** of a matrix, perhaps the single most important number associated with a matrix. That's a discussion for another day.\n",
    "\n",
    ":::{attention} The Two Pictures\n",
    "\n",
    "Any time you see a matrix-vector product, like $A \\vec v$, you should think of it not as a random operation, but as:\n",
    "\n",
    "1. (**More Important**) A linear combination of the columns of $A$, where the weights are the components of $\\vec v$.\n",
    "$$A \\vec v = v_1 \\left( \\text{column } 1 \\text{ of } A \\right) + v_2 \\left( \\text{column } 2 \\text{ of } A \\right) + \\ldots + v_d \\left( \\text{column } d \\text{ of } A \\right)$$\n",
    "\n",
    "2. A dot product of the rows of $A$ with $\\vec v$.\n",
    "$$A \\vec v = \\begin{bmatrix} (\\text{row 1 of } A) \\cdot \\vec v \\\\ (\\text{row 2 of } A) \\cdot \\vec v \\\\ \\vdots \\\\ (\\text{row n of } A) \\cdot \\vec v \\end{bmatrix}$$\n",
    ":::\n",
    "\n",
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Consider the matrix $M$ defined below.\n",
    "\n",
    "$$M = \\begin{bmatrix} 2 & -1 & 3 & 0 & 4 \\\\ 1 & 5 & -2 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "In each of the following parts, (1) write out $\\vec u$ concretely, (2) compute $M \\vec u$, and (3) explain the result in relation to the linear combination interpretation of matrix-vector multiplication.\n",
    "\n",
    "- A vector whose second component is 1, and whose other components are 0.\n",
    "- A vector containing all 1s.\n",
    "- A vector containing all $\\frac{1}{5}$s.\n",
    "- A vector whose components sum to 1, whose first component is $\\frac{3}{5}$, and whose other components are all equal to one another.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Matrix Multiplication\n",
    "\n",
    "Matrix-matrix multiplication is a generalization of matrix-vector multiplication. Let's present matrix-matrix multiplication in its most general terms.\n",
    "\n",
    ":::{note} Definition: Matrix-Matrix Multiplication\n",
    "Suppose:\n",
    "- $A$ is a $n \\times d$ matrix.\n",
    "- $B$ is a $d \\times p$ matrix.\n",
    "\n",
    "Then, $AB$ is a $n \\times p$ matrix such that \n",
    "- the element in row $i$ and column $j$ of $AB$ is \n",
    "- the **dot product of row $i$ of $A$ and column $j$ of $B$**, for $i = 1, 2, ..., n$ and $j = 1, 2, ..., p$.\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$(AB)_{ij} = \\left( \\text{row } i \\text{ of } A \\right) \\cdot \\left( \\text{column } j \\text{ of } B \\right) = \\sum_{k=1}^d A_{ik} B_{kj}$$\n",
    ":::\n",
    "\n",
    "Note that if $p = 1$, this reduces to the matrix-vector multiplication case from before. In that case, the only possible value of $j$ is 1, since the output only has 1 column, and the element in row $i$ of the output vector is the dot product of row $i$ in $A$ and the vector $B$ (which we earlier referred to as $\\vec v$ in the less general case).\n",
    "\n",
    "For a concrete example, suppose $A$ and $B$ are defined below:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\quad B = \\begin{bmatrix} 1 & 2\\\\ 0 & 7\\\\ 3 & 2 \\end{bmatrix}$$\n",
    "\n",
    "The number of columns of $A$ must equal the number of rows of $B$ in order for the product $AB$ to be defined, as the Golden Rule tells us. That is fortunately the case here. Since $A$ has shape $4 \\times 3$ and $B$ has shape $3 \\times 2$, the output matrix will have shape $4 \\times 2$. Each of those 8 elements will be the dot product of a row in $A$ with a column in $B$.\n",
    "\n",
    "Here is the product of $A$ and $B$:\n",
    "\n",
    "$$AB = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 2\\\\ 0 & 7\\\\ 3 & 2 \\end{bmatrix} = \\begin{bmatrix} 15 & 21 \\\\ 29 & 29 \\\\ 0 & -7 \\\\ 2 & -10 \\end{bmatrix}$$\n",
    "\n",
    "Let's see if we can audit where these numbers came from. Let's consider $(AB)_{32}$, which is the element in row 3 and column 2 of the output. It should have come from the dot product of row 3 of $A$ and column 2 of $B$.\n",
    "\n",
    "$$AB = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ \\color{#3d81f6} \\mathbf{0} & \\color{#3d81f6} \\mathbf{-1} & \\color{#3d81f6} \\mathbf{0} \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & \\color{#3d81f6} \\mathbf{2}\\\\ 0 & \\color{#3d81f6} \\mathbf{7}\\\\ 3 & \\color{#3d81f6} \\mathbf{2} \\end{bmatrix} = \\begin{bmatrix} 15 & 21 \\\\ 29 & 29 \\\\ 0 & \\boxed{\\color{#3d81f6} \\mathbf{-7}} \\\\ 2 & -10 \\end{bmatrix}$$\n",
    "\n",
    "And indeed, $-7$ is the dot product of $\\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 2 \\\\ 7 \\\\ 2 \\end{bmatrix}$, as $0 \\cdot 2 + (-1) \\cdot 7 + 0 \\cdot 2 = -7$.\n",
    "\n",
    "You should notice that many of the numbers in the output $AB$ look familiar. That's because we used the same $A$ as we did earlier in the section, and the first column of $B$ is just $\\vec v$ from the matrix-vector example. So, the first column in $AB$ is the same as the vector $M \\vec v = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2\\end{bmatrix}$ as we computed earlier. The difference now is that the output $AB$ isn't just a single vector, but is a matrix with 2 columns. The second column, $\\begin{bmatrix} 21 \\\\ 29 \\\\ -7 \\\\ -10\\end{bmatrix}$, comes from multiplying $A$ by the second column in $B$, namely $\\begin{bmatrix} 2 \\\\ 7 \\\\ 2\\end{bmatrix}$.\n",
    "\n",
    "Note that as we add columns to $B$, we'd add columns to the output. If $B$ had 10 columns, then $AB$ would have 10 columns, too, without $A$ needing to change. As long as the Golden Rule – that the number of columns in $A$ equals the number of rows in $B$ – holds, the product $AB$ can be computed, and it has shape $(\\text{number of rows in } A) \\times (\\text{number of columns in } B)$.\n",
    "\n",
    "In `numpy`, the `@` operator is used for matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=B = np.array([[1, 2], [0, 7], [3, 2]])&code=A&code=B&code=A @ B # Shorthand for np.matmul(A, B)\"\n",
    "  width=\"100%\"\n",
    "  height=\"500px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning} Matrix Multiplication is Not Commutative!\n",
    "\n",
    "**In general, $AB \\neq BA$.**\n",
    "\n",
    " This may come as a surprise, as every other form of multiplication you've learned about up until this point has been commutative.\n",
    "\n",
    "In fact, if $AB$ exists, $BA$ may or may not! If $A$ is $n \\times d$ and $B$ is $d \\times p$, then $BA$ only exists if $n = p$. But even then, $AB \\neq BA$ in general.\n",
    "\n",
    "- For example, if $A$ is $2 \\times 3$ and $B$ is $3 \\times 2$, then $AB$ is $2 \\times 2$ and $BA$ is $3 \\times 3$; here, both products exist, but they cannot be equal since they have different shapes.\n",
    "- Even if $A$ and $B$ are both $n \\times n$, $AB \\neq BA$ in general. For illustration, consider $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$ and $B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}$, then $AB = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}$ and $BA = \\begin{bmatrix} 23 & 34 \\\\ 31 & 46 \\end{bmatrix}$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Something about permutation matrices\n",
    "\n",
    "Something about the Fibonacci sequence\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've shown you the naïve – and by far most common – algorithm for matrix multiplication. If $A$ and $B$ are both square $n \\times n$ matrices, then the runtime of the naïve algorithm is $O(n^3)$.\n",
    "\n",
    "However, there exist more efficient algorithms for matrix multiplication. Strassen's algorithm is one such example; it describes how to multiply two square $n \\times n$ matrices in $O(n^{2.807})$ time. The study of efficient algorithms for matrix multiplication is an active area of research; if you're interested in learning more, look [here](https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication).\n",
    "\n",
    "Matrix multiplication, I might argue, is one of the reasons NVIDIA is the most valuable company in the world. Modern machine learning is built on matrix multiplication, and GPUs are optimized for it. Why? This comment [from Reddit](https://www.reddit.com/r/explainlikeimfive/comments/zpso6w/eli5_what_about_gpu_architecture_makes_them/) does a good job of explaining:\n",
    "\n",
    "> Imagine you have 1 million math assignments to do, they are very simple assignments, but there are a lot that need to be done, they are not dependent on each other so they can be done on any order.\n",
    "> \n",
    "> You have two options, distribute them to 10 thousand people to do it in parallel or give them to 10 math experts. The experts are very fast, but hey, there are only 10 of them, the 10 thousand are more suitable for the task because they have the \"brute force\" for this.\n",
    ">\n",
    "> GPUs have thousands of cores, CPUs have tens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transpose\n",
    "\n",
    "There's an important operation on matrices that we haven't discussed yet.\n",
    "\n",
    ":::{note} Definition: Transpose\n",
    "\n",
    "The **transpose** of a matrix $A$ is the matrix $A^T$ with entries $A^T_{ij} = A_{ji}$ for all $i, j$.\n",
    "\n",
    "Intuitively, the transpose results from replacing the rows of $A$ with the columns of $A$ and vice-versa.\n",
    ":::\n",
    "\n",
    "To illustrate, let's start with our familiar matrix $A$:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & {\\color{#3d81f6} \\mathbf{1}} & 4 \\\\ 2 & {\\color{#3d81f6} \\mathbf{1}} & 9 \\\\ 0 & {\\color{#3d81f6} \\mathbf{-1}} & 0 \\\\ 2 & {\\color{#3d81f6} \\mathbf{-2}} & 0 \\end{bmatrix}$$\n",
    "\n",
    "The transpose of $A$ is:\n",
    "\n",
    "$$A^T = \\begin{bmatrix} 3 & 2 & 0 & 2 \\\\ {\\color{#3d81f6} \\mathbf{1}} & {\\color{#3d81f6} \\mathbf{1}} & {\\color{#3d81f6} \\mathbf{-1}} & {\\color{#3d81f6} \\mathbf{-2}} \\\\ 4 & 9 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Note that $A \\in \\mathbb{R}^{4 \\times 3}$ and $A^T \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "Why would we ever need to do this? Let's think in terms of the \"two pictures\" defined earlier in the section. Since the rows of $A$ are the columns of $A^T$, you can interpret $A^T \\vec u$ as:\n",
    "\n",
    "1. A linear combination of the **rows of $A$** (which are the columns of $A^T$), where the weights are the components of $\\vec u$.\n",
    "$$A^T \\vec u = u_1 \\left( \\text{row } 1 \\text{ of } A \\right) + u_2 \\left( \\text{row } 2 \\text{ of } A \\right) + \\ldots + u_n \\left( \\text{row } n \\text{ of } A \\right)$$\n",
    "\n",
    "2. A dot product of the **columns of $A$** (which are the rows of $A^T$) with $\\vec u$.\n",
    "$$A^T \\vec u = \\begin{bmatrix} (\\text{column 1 of } A) \\cdot \\vec u \\\\ (\\text{column 2 of } A) \\cdot \\vec u \\\\ \\vdots \\\\ (\\text{column d of } A) \\cdot \\vec u \\end{bmatrix}$$\n",
    "\n",
    "Suppose $\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\end{bmatrix}$ (note that $\\vec u$ must be in $\\mathbb{R}^4$ in order for $A^T \\vec u$ to be defined, unlike $\\vec v \\in \\mathbb{R}^3$ in the product $A \\vec v$). Then:\n",
    "\n",
    "$$A^T \\vec u = \\begin{bmatrix} 3 & 2 & 0 & 2 \\\\ 1 & 1 & -1 & -2 \\\\ 4 & 9 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} {\\color{orange} \\mathbf{u_1}} \\\\ {\\color{orange} \\mathbf{u_2}} \\\\ {\\color{orange} \\mathbf{u_3}} \\\\ {\\color{orange} \\mathbf{u_4}} \\end{bmatrix} = {\\color{orange} \\mathbf{u_1}} \\begin{bmatrix} 3 \\\\ 1 \\\\ 4 \\end{bmatrix} + {\\color{orange} \\mathbf{u_2}} \\begin{bmatrix} 2 \\\\ 1 \\\\ 9 \\end{bmatrix} + {\\color{orange} \\mathbf{u_3}} \\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix} + {\\color{orange} \\mathbf{u_4}} \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "This is a linear combination of the rows of $A$, where the weights are the components of $\\vec u$. Remember, the standard product $A \\vec v$ is a linear combination of the columns of $A$, so the transpose helps us if we want to compute a linear combination of the rows of $A$.\n",
    "\n",
    "The transpose also gives us another way of expressing the dot product of two vectors. If $\\vec u$ and $\\vec v$ are two vectors in $\\mathbb{R}^n$, then $\\vec u^T$ is a row vector with 1 row and $n$ columns. Multiplying $\\vec u^T$ by $\\vec v$ results in a $1 \\times 1$ matrix, which is just the scalar $\\vec u \\cdot \\vec v$.\n",
    "\n",
    "$$\n",
    "\\vec {\\color{#3d81f6}u}^T \\vec{\\color{purple}v} = \\begin{bmatrix} {\\color{#3d81f6}u_1} & {\\color{#3d81f6}u_2} & \\ldots & {\\color{#3d81f6}u_n} \\end{bmatrix} \\begin{bmatrix}{\\color{purple}v_1} \\\\{\\color{purple}v_2} \\\\ \\vdots \\\\{\\color{purple}v_n} \\end{bmatrix} = {\\color{#3d81f6}u_1}{\\color{purple}v_1} + {\\color{#3d81f6}u_2}{\\color{purple}v_2} + \\ldots + {\\color{#3d81f6}u_n}{\\color{purple}v_n} = \\vec {\\color{#3d81f6}u} \\cdot \\vec{\\color{purple}v} = \\vec{\\color{purple}v} \\cdot \\vec {\\color{#3d81f6}u} = \\vec{\\color{purple}v}^T \\vec {\\color{#3d81f6}u}\n",
    "$$\n",
    "\n",
    ":::{warning} Transpose, or Dot, but Not Both!\n",
    "\n",
    "The following are all valid ways of computing the dot product of $\\vec u$ and $\\vec v$, assuming they have the same number of components:\n",
    "\n",
    "$$\n",
    "\\vec u \\cdot \\vec v = \\vec u^T \\vec v = \\vec v^T \\vec u = \\vec v \\cdot \\vec u\n",
    "$$\n",
    "\n",
    "However, $\\vec u^T \\cdot \\vec v$ **is not defined**. The dot product is only defined for two vectors of the same dimensions, but $\\vec u^T$ is a row vector with $n$ columns, and $\\vec v$ is a column vector with $n$ rows.\n",
    "\n",
    ":::\n",
    "\n",
    "The benefit of using the transpose to express the dot product is that it allows us to write the dot product of two vectors in terms of matrix multiplication, rather than being an entirely different type of operation. (In fact, as we've seen here, matrix multiplication is just a generalization of the dot product.)\n",
    "\n",
    "There are other uses for the transpose, too, so it's a useful tool to have in your toolbox. Here are some useful properties of the transpose:\n",
    "\n",
    "- $(A^T)^T = A$, i.e. the transpose of the transpose of a matrix is the original matrix.\n",
    "- $(A + B)^T = A^T + B^T$.\n",
    "- $(cA)^T = cA^T$ for any scalar $c \\in \\mathbb{R}$.\n",
    "- $(AB)^T = B^T A^T$.\n",
    "\n",
    "The first three properties are relatively straightforward. The last property is a bit more subtle. Try and reason as to why it's true on your own, then peek into the box below to verify your reasoning.\n",
    "\n",
    ":::{admonition} Why is $(AB)^T = B^T A^T$?\n",
    ":class: dropdown\n",
    "\n",
    "Let's start with just $(AB)^T$ and reason our way from there. Define $C = (AB)^T$. $C$ is presumably the product of two matrices $X$ and $Y$, we just don't know what $X$ and $Y$ are. By the definition of matrix multiplication, we know that $C_{ij}$ is the dot product of the $i$th row of $X$ and the $j$th column of $Y$. How can we express the product $C = XY$ in terms of $A$ and $B$?\n",
    "\n",
    "Let's work backwards. Since $C_{ij} = (AB)^T_{ij} = (AB)_{ji}$ by the definition of the transpose, we know that: \n",
    "\n",
    "$$C_{ij} = (AB)_{ji} = \\left( \\text{row } j \\text{ of } A \\right) \\cdot \\left( \\text{column } i \\text{ of } B \\right) = \\left( \\text{column } i \\text{ of } B \\right) \\cdot \\left( \\text{row } j \\text{ of } A \\right)$$\n",
    "\n",
    "This is a little backwards relative to the definition of matrix multiplication, which says that:\n",
    "\n",
    "$$C_{ij} = (XY)_{ij} = \\left( \\text{row } i \\text{ of } X \\right) \\cdot \\left( \\text{column } j \\text{ of } Y \\right)$$\n",
    "\n",
    "In order for the two definitions of $C_{ij}$ to be consistent, we must have:\n",
    "\n",
    "$$\\left( \\text{column } i \\text{ of } B \\right) \\cdot \\left( \\text{row } j \\text{ of } A \\right) = \\left( \\text{row } i \\text{ of } X \\right) \\cdot \\left( \\text{column } j \\text{ of } Y \\right)$$\n",
    "\n",
    "- Row $i$ of $X$ is the same as column $i$ of $B$, if $X = B^T$.\n",
    "- Column $j$ of $Y$ is the same as row $j$ of $A$, if $Y = A^T$.\n",
    "\n",
    "Putting this together, we have:\n",
    "\n",
    "$$C = (AB)^T = B^T A^T$$\n",
    "\n",
    "as we hoped!\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In `numpy`, the `T` attribute is used to compute the transpose of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=A&code=A.T\"\n",
    "  width=\"100%\"\n",
    "  height=\"175%\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Suppose $M \\in \\mathbb{R}^{n \\times d}$ is a matrix, $\\vec{x} \\in \\mathbb{R}^d$ is a vector, and $s \\in \\mathbb{R}$ is a scalar.\n",
    "\n",
    "Determine whether each of the following quantities is a matrix, vector, scalar, or undefined. If the result is a matrix or vector, determine its dimensions.\n",
    "\n",
    "1. $M\\vec{x}$\n",
    "\n",
    "2. $\\vec{x} M$\n",
    "\n",
    "3. $\\vec{x}^2$\n",
    "\n",
    "4. $M^TM$\n",
    "\n",
    "5. $MM^T$\n",
    "\n",
    "6. $\\vec{x}^T M \\vec{x}$\n",
    "\n",
    "7. $(sM\\vec{x}) \\cdot (sM\\vec{x})$\n",
    "\n",
    "8. $(s \\vec{x}^T M^T)^T$\n",
    "\n",
    "9. $\\vec{x}^T M^T M \\vec{x}$\n",
    "\n",
    "10. $\\vec{x}\\vec{x}^T + M^TM$\n",
    "\n",
    "11. $\\frac{M \\vec{x}}{\\lVert \\vec{x} \\rVert} + (\\vec{x}^T M^T M \\vec{x}) M \\vec{x}$\n",
    "\n",
    "TODO something about the quadratic form, $f(\\vec{x}) = \\vec{x}^T A \\vec{x}$, have them work it out.\n",
    "\n",
    "have them find the transpose of (ABC)^T or something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Matrices\n",
    "\n",
    "To wrap up this section, we'll outline a few types of matrices one should know about. Each of these types of matrices have unique properties that make them useful to us. It may take until Chapter 5 (on Eigenvalues and Eigenvectors) to see the full spectrum of their uses, but it's good to know about them now.\n",
    "\n",
    "### Identity Matrix\n",
    "\n",
    ":::{admonition} Definition: Identity Matrix\n",
    "\n",
    "The **identity matrix** is the square matrix $I$ with ones on the diagonal and zeros everywhere else.\n",
    "\n",
    "$$I = \\begin{bmatrix} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}$$\n",
    "\n",
    ":::\n",
    "\n",
    "Saying \"the identity matrix\" is a bit ambiguous, as there are infinitely many identity matrices – there's a $1 \\times 1$ identity matrix, a $2 \\times 2$ identity matrix, a $3 \\times 3$ identity matrix, and so on. Often, the dimension of the identity matrix is implied by context, and if not, we might provide it as a subscript, e.g. $I_n$ for the $n \\times n$ identity matrix.\n",
    "\n",
    "Why is the identity matrix defined this way? It's the matrix equivalent of the number $1$ in scalar multiplication, also known as the multiplicative identity. If $c$ is a scalar, then $c \\cdot 1 = c$ and $1 \\cdot c = c$. \n",
    "\n",
    "Similarly, if $A$ is **square** $n \\times n$ matrix and $\\vec v \\in \\mathbb{R}^n$ is a vector, then the $n \\times n$ identity matrix is the unique matrix that satisfies:\n",
    "- $I \\vec v = \\vec v$ for all $\\vec v \\in \\mathbb{R}^n$.\n",
    "- $I A = A I = A$ for all $A \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "A good exercise is to verify that the identity matrix satisfies these properties, using the definition of matrix multiplication.\n",
    "\n",
    "### Symmetric Matrices\n",
    "\n",
    ":::{admonition} Definition: Symmetric Matrix\n",
    "\n",
    "A matrix $S$ is **symmetric** if $S = S^T$.\n",
    ":::\n",
    "\n",
    "Note that only square matrices can be symmetric, as the transpose of a $n \\times d$ matrix is a $d \\times n$ matrix.\n",
    "\n",
    "Just so we see one, here's an example of a symmetric matrix:\n",
    "\n",
    "$$S = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}$$\n",
    "\n",
    "Symmetric matrices will appear in increasing regularity from here on out, as we start to move back to the study of linear regression. One key symmetric matrix that appears quite frequently is the matrix $X^T X$, where $X$ is **any** $n \\times d$ matrix. That's right: even if $X$ isn't square, $X^T X$ is symmetric! We can verify that this is true by using the properties of the transpose:\n",
    "\n",
    "$$(X^T X)^T = X^T (X^T)^T = X^T X$$\n",
    "\n",
    "Since $X^T$ is $d \\times n$ and $X$ is $n \\times d$, $X^T X$ is $d \\times d$. This also tells us that $XX^T$ is a $n \\times n$ matrix, and it is also symmetric, as can be verified using the logic above.\n",
    "\n",
    "What is $X^T X$? Let's denote the $j$th column of $X$ as $X_{:, j}$. (This is a bit of notational abuse, but I want to make sure to not use $\\vec x_i$ to denote the $i$th column of $X$, as we've gotten used to referring to the $i$th observation of a dataset as $\\vec x_i$.)\n",
    "\n",
    "Then, we can write $X^T X$ as:\n",
    "\n",
    "$$\\begin{align*} X^TX &= \\begin{bmatrix} - X_{:, 1}^T - \\\\ - X_{:, 2}^T - \\\\ \\vdots \\\\ - X_{:, d}^T - \\end{bmatrix} \\begin{bmatrix} \\mid & \\mid & \\mid \\\\  X_{:, 1} &  X_{:, 2} & \\ldots &  X_{:, d} \\\\ \\mid & \\mid & \\mid \\end{bmatrix} \\\\ &= \\begin{bmatrix}  X_{:, 1} \\cdot  X_{:, 1} &  X_{:, 1} \\cdot  X_{:, 2} & \\ldots &  X_{:, 1} \\cdot  X_{:, d} \\\\  X_{:, 2} \\cdot  X_{:, 1} &  X_{:, 2} \\cdot  X_{:, 2} & \\ldots &  X_{:, 2} \\cdot  X_{:, d} \\\\ & \\vdots & \\\\  X_{:, d} \\cdot  X_{:, 1} &  X_{:, d} \\cdot  X_{:, 2} & \\ldots &  X_{:, d} \\cdot  X_{:, d} \\end{bmatrix}\\end{align*}$$\n",
    "\n",
    "This is a $d \\times d$ matrix, where the entry at position $(i, j)$ is the dot product of the $i$th column of $X$ and the $j$th column of $X$. This matrix is sometimes called the **Gram matrix** of $X$.\n",
    "\n",
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Consider the $5 \\times 2$ matrix $X$, defined below.\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & -2 \\\\ -1 & 3 \\\\ 2 & 0 \\\\ 0 & -1 \\\\ 3 & 2 \\end{bmatrix}$$\n",
    "\n",
    "Compute $X^TX + \\frac{1}{2} I$. We'll use matrices of the form $X^TX + \\lambda I$ in Chapter 5.\n",
    "\n",
    ":::\n",
    "\n",
    "### Diagonal Matrices\n",
    "\n",
    ":::{admonition} Definition: Diagonal Matrix\n",
    "\n",
    "A square matrix $D$ is **diagonal** if all entries off the diagonal are zero. In other words, $D_{ij} = 0$ for all $i \\neq j$.\n",
    "\n",
    "$$D = \\begin{bmatrix} d_1 & 0 & \\ldots & 0 \\\\ 0 & d_2 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & d_n \\end{bmatrix}$$\n",
    "\n",
    ":::\n",
    "\n",
    "The identity matrix is a diagonal matrix. As another example, here's another $4 \\times 4$ diagonal matrix:\n",
    "\n",
    "$$D = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & -9 & 0 \\\\ 0 & 0 & 0 & \\frac{1}{2} \\end{bmatrix}$$\n",
    "\n",
    "Diagonal matrices become useful when studying powers of matrices, like $A^2$ or $A^3$, which we'll need to do in Chapter 5 when we learn about how Google's search algorithm works. If $A$ is an arbitrary $n \\times n$ matrix, then computing $A^2 = AA$ requires us to compute a full matrix multiplication. But, if $D$ is diagonal, then $D^2$ is just a diagonal matrix with each entry squared:\n",
    "\n",
    "$$D^2 = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 81 & 0 \\\\ 0 & 0 & 0 & \\frac{1}{4} \\end{bmatrix}$$\n",
    "\n",
    "Unless otherwise specified, diagonal matrices are assumed to be square. But, we'll occasionally see non-square diagonal matrices, like the $3 \\times 4$ matrix:\n",
    "\n",
    "$$D = \\begin{bmatrix} 5 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -9 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "This is a diagonal $5 \\times 3$ matrix. All entries $D_{ij}$ where $i \\neq j$ are zero. These will come up in Chapter 5 when we study the singular value decomposition, which will unlock a powerful unsupervised machine learning technique. (Aren't you excited for it?)\n",
    "\n",
    "Related to the idea of a diagonal matrix is that of an upper triangular matrix. A square matrix $U$ is **upper triangular** if all entries below the diagonal are zero. In other words, $U_{ij} = 0$ for all $i > j$.\n",
    "\n",
    "$$U = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 6 \\end{bmatrix}$$\n",
    "\n",
    "Similarly, a square matrix $L$ is **lower triangular** if all entries above the diagonal are zero, i.e. $L_{ij} = 0$ for all $i < j$.\n",
    "\n",
    "$$L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 3 & 0 \\\\ 4 & 5 & 6 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    ":::{tip} Exercise\n",
    ":class: dropdown\n",
    "\n",
    "Do an exercise that has them derive the idea of an orthogonal matrix, and how they preserve the norm of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
