{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "numbering: false\n",
    "---\n",
    "\n",
    "# 2.5. Matrices\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sections, we learned to consider the span of a set of vectors, and alluded to the fact that to solve the regression problem in higher dimensions, we'd need to be able to project a vector onto the span of a set of vectors. Matrices will allow us to achieve this goal elegantly. It will take a few sections to get there:\n",
    "\n",
    "- In Chapter 2.5, we'll define matrices, and learn to perform basic operations on them.\n",
    "- In Chapter 2.6, we'll define the rank of a matrix, and better understand the span of the columns of a matrix.\n",
    "- In Chapter 2.7, we'll understand matrix-vector multiplication as a transformation of the vector, and learn how to invert a matrix and compute its determinant.\n",
    "- In Chapter 2.8, we'll learn to project a vector onto the span of the columns of a matrix, closing the loop with Chapter 2.3.\n",
    "\n",
    "For now, let's consider the following three vectors in $\\mathbb{R}^4$:\n",
    "\n",
    "$$\\vec u_1 = \\begin{bmatrix} 3 \\\\ 2 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\vec u_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\quad \\vec u_3 = \\begin{bmatrix} 4 \\\\ 9 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "If we stack these vectors horizontally, we produce a **matrix**:\n",
    "\n",
    "$$\\begin{bmatrix} \\mid & \\mid & \\mid \\\\ \\vec u_1 & \\vec u_2 & \\vec u_3 \\\\ \\mid & \\mid & \\mid \\end{bmatrix} = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    ":::{note} Definition: Matrix\n",
    "A **matrix** is a rectangular array of numbers, organized into rows and columns.\n",
    "\n",
    "In this class, we'll typically use uppercase letters to denote matrices. For example:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ is a matrix with 4 rows and 3 columns, so we might say that $A$ is a $4 \\times 3$ matrix, or that $A \\in \\mathbb{R}^{4 \\times 3}$.\n",
    "\n",
    "It's common to use the notation $A_{ij}$ to denote the entry in the $i$-th row and $j$-th column of $A$, e.g. $A_{23} = 9$. \n",
    "\n",
    "In `numpy`, you can access the entry in the 2nd row and 3rd column of the 2D matrix `A` using `A[1, 2]`, once we account for the fact that `numpy` uses 0-based indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=A&code=A[1, 2] # Remember that Python uses 0-based indexing!\"\n",
    "  width=\"100%\"\n",
    "  height=\"175%\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip} Activity 1\n",
    ":class: dropdown\n",
    "\n",
    "In the cell above, try and find the bottom-right entry of `A`, first using positive indexing and then using negative indexing.\n",
    ":::\n",
    "\n",
    "I've introduced matrices in a very particular way, because I'd like for you to think of them as a collection of vectors, sometimes called **column vectors**. We'll come back to this in just a moment. You can also think of a matrix as a collection of row vectors; the example $A$ defined above consists of four row vectors in $\\mathbb{R}^3$ stacked horizontally.\n",
    "\n",
    "When we introduced vectors, $n$ was typically the placeholder we'd use for the number of components of a vector. With matrices, I like using $n$ for the number of rows, and $d$ for the number of columns. This means that, in general, a matrix is of shape $n \\times d$ and is a member of the set $\\mathbb{R}^{n \\times d}$. **This makes clear that when we've collected data, we store each of our $n$ observations in a row of our matrix.** Just be aware that using $n \\times d$ to denote the shape of a matrix is a little unorthodox; most other textbooks will use $m \\times n$ to denote the shape of a matrix. Of course, this is all arbitrary.\n",
    "\n",
    "<!-- If $A$ is an arbitrary matrix with $n$ rows and $d$ columns, i.e. $A \\in \\mathbb{R}^{n \\times d}$, we can write $A$ as:\n",
    "\n",
    "$$A = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1d} \\\\ a_{21} & a_{22} & \\cdots & a_{2d} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nd} \\end{bmatrix}$$\n",
    "\n",
    "We often use the notation $a_{ij}$ to denote the entry in the $i$-th row and $j$-th column of $A$. -->\n",
    "\n",
    "\n",
    "Suppose $A$ has $n$ rows and $d$ columns, i.e. $A \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "- If $n > d$, we say that $A$ is **tall**.\n",
    "- If $n = d$, we say that $A$ is **square**.\n",
    "- If $n < d$, we say that $A$ is **wide**.\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix} 5 & 3 \\\\ 2 & 1 \\\\ -1 & \\frac{1}{3} \\\\ 3 & 6 \\\\ 0 & 1 \\end{bmatrix}}_{\\text{tall}} \\qquad \\underbrace{\\begin{bmatrix} 3 & 0 & 4 \\\\ 4 & -\\pi & 0 \\\\ \\frac{1}{2} & 0 & 1 \\end{bmatrix}}_{\\text{square}} \\qquad \\underbrace{\\begin{bmatrix} 1 & 0 & 3 & 2 & -1 \\\\ \\frac{1}{9} & 3 & 0 & 0 & 2 \\\\ 9 & 0 & 0 & 6 & -3 \\end{bmatrix}}_{\\text{wide}}$$\n",
    "\n",
    "As we'll see in the coming sections, square matrices are the most flexible – there are several properties and operations that are only defined for them. Unfortunately, not every matrix is square. Remember, matrices are used for storing data, and the number of observations, $n$ and number of features, $d$, don't need to be the same. (In practice, we'll often have very tall matrices, i.e. $n \\gg d$, but this is not always the case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Addition and Scalar Multiplication\n",
    "\n",
    "Like vectors, matrices support addition and scalar multiplication out-of-the-box, and both behave as you'd expect.\n",
    "\n",
    ":::{note} Definition: Addition\n",
    "\n",
    "Suppose $A$ and $B$ are matrices with the same shape, i.e. $A, B \\in \\mathbb{R}^{n \\times d}$. Then, the **sum** of $A$ and $B$ is the matrix $C$ with entries $C_{ij} = A_{ij} + B_{ij}$ for all $i, j$.\n",
    "\n",
    "That is, the sum is performed element-wise. The sum is also commutative: $$A + B = B + A$$\n",
    ":::\n",
    "\n",
    ":::{note} Definition: Scalar Multiplication\n",
    "\n",
    "Suppose $A$ is a matrix and $c$ is a scalar. Then, the **scalar multiple** of $A$ by $c$ is the matrix $B$ with entries $B_{ij} = c A_{ij}$ for all $i, j$.\n",
    "\n",
    ":::\n",
    "\n",
    "Let's see an example. Consider the matrices $A$ and $B$ (where $A$ is the same as in the previous example):\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix}$$\n",
    "\n",
    "Then, the operation $3A - B$ is well-defined, and produces the matrix:\n",
    "\n",
    "$$3A - B = 3 \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & {\\color{#3d81f6} \\mathbf{-1}} & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & \\color{#3d81f6} \\mathbf{8} & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 8 & 1 & 9 \\\\ 2 & -2 & 21 \\\\ -7 & \\boxed{\\color{#3d81f6} \\mathbf{-11}} & -9 \\\\ -4 & -17 & -12 \\end{bmatrix}$$\n",
    "\n",
    "I've colored the entry at position $(3, 2)$ $\\color{#3d81f6} \\text{blue}$ to help you trace the computation:\n",
    "\n",
    "$$3 \\cdot (-1) - 8 = -11$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Matrix-Vector Multiplication\n",
    "\n",
    "Great – we know how to add two matrices, and how to multiply a matrix by a scalar. The natural next step is to figure out how to multiply two matrices together, and to understand _why_ we might do so. (Hopefully, the \"why\" has something to do with data.)\n",
    "\n",
    "First, a definition.\n",
    "\n",
    ":::{attention} Golden Rule of Matrix Multiplication\n",
    "Suppose $A$ and $B$ are two matrices. In order for the product $AB$ to be valid, **the number of columns in $A$ must equal the number of rows in $B$**. \n",
    "\n",
    "$$\\text{\\# columns in } A = \\text{\\# rows in } B$$\n",
    "\n",
    "In other words, **the inner dimensions of $A$ and $B$ must match**.\n",
    ":::\n",
    "\n",
    "Let's use the Golden Rule. First – as the title of this subsection suggests – we'll start by computing the product of a matrix and a vector.\n",
    "\n",
    "Let's suppose $A$ is the same $4 \\times 3$ matrix we've become familiar with:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix}$$\n",
    "\n",
    "And let's suppose $\\vec v$ is some vector. Note that we can think of an $n$-dimensional vector as a matrix with $n$ rows and 1 column. In order for the product $A \\vec v$ to be valid, $\\vec v$ must have 3 elements in it, by the Golden Rule above. To make the example concrete, let's suppose:\n",
    "\n",
    "$$\\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "How do we multiply $A \\vec v$? The following key definition will help us.\n",
    "\n",
    ":::{note} Definition: Matrix-Vector Multiplication\n",
    "\n",
    "If $A$ is a $n \\times d$ matrix and $\\vec v \\in \\mathbb{R}^d$ is a vector, then the product $A \\vec v$ is a vector in $\\mathbb{R}^n$ that contains **the dot product of each _row_ of $A$ with $\\vec v$**.\n",
    ":::\n",
    "\n",
    "<!-- $A$ has 4 rows, and $\\vec v$ has 1 column (itself), and so $A \\vec v$ will be a vector with 4 components, made up of 4 dot products. -->\n",
    "\n",
    "Let me say a little more about the dimensions of the output. Below, I've written the dimensions of $A$ ($4 \\times 3$) and $\\vec v$ ($3 \\times 1$) next to each other. By the Golden Rule, the **inner dimensions, both of which are bolded**, must be equal in order for the multiplication to be valid. The dimensions of the output will be the result of looking at the $\\boxed{\\text{outer dimensions}}$, which here are $4 \\times 1$.\n",
    "\n",
    "$$\n",
    "\\underbrace{A}_{\\boxed{4} \\times \\textbf{3}} \\:\\:\\:\\: \\underbrace{\\vec v}_{\\textbf{3} \\times \\boxed{1}} = \\underbrace{\\text{output}}_{4 \\times 1}\n",
    "$$\n",
    "\n",
    "So, the result of multiplying $A \\vec v$ will be $4 \\times 1$ matrix, or in other words, a vector in $\\mathbb{R}^4$. Indeed, the result of multiply a matrix by a vector always results in another vector, and this act of multiplying a matrix by a vector is often thought of as **transforming** the vector from $\\mathbb{R}^d$ to $\\mathbb{R}^n$.\n",
    "\n",
    "So, how do we find those 4 components? As mentioned earlier, we compute each component by taking the dot product of a row in $A$ with $\\vec v$.\n",
    "\n",
    "$$A = \\begin{bmatrix} \\color{#3d81f6} \\mathbf{3} & \\color{#3d81f6} \\mathbf{1} & \\color{#3d81f6} \\mathbf{4} \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\quad \\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "Let's start with the top row of $A$ (colored blue above), which is $\\begin{bmatrix} 3 \\\\ 1 \\\\ 4\\end{bmatrix}$. The dot product of two vectors is only defined if they have equal lengths. **This is why we've instituted the Golden Rule!** The Golden Rule tells us we can only multiply $A$ and $\\vec v$ if the number of columns in $A$ is the same as the number of components in $\\vec v$, which is true here (both of those numbers are 3).\n",
    "\n",
    "Then, the dot product of the first row of $A$ with $\\vec v$ is:\n",
    "\n",
    "$$\\begin{bmatrix} \\color{#3d81f6} \\mathbf{3} \\\\ \\color{#3d81f6} \\mathbf{1} \\\\ \\color{#3d81f6} \\mathbf{4} \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = {\\color{#3d81f6} \\mathbf{3}} \\cdot 1 + {\\color{#3d81f6} \\mathbf{1}} \\cdot 0 + {\\color{#3d81f6} \\mathbf{4}} \\cdot 3 = 15$$\n",
    "\n",
    "Nice! We're a quarter of the way there. Now, we just need to compute the remaining three dot products:\n",
    "\n",
    "- The dot product of the second row of $A$ with $\\vec v$ is $\\begin{bmatrix} 2 \\\\ 1 \\\\ 9 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 29$.\n",
    "\n",
    "- The dot product of the third row of $A$ with $\\vec v$ is $\\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 0$.\n",
    "\n",
    "- And finally, the dot product of the fourth row of $A$ with $\\vec v$ is $\\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = 2$.\n",
    "\n",
    "The result of our matrix-vector multiplication, then, is the result of stacking all 4 dot products together into the vector $\\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2\\end{bmatrix}$. To summarize:\n",
    "\n",
    "$$A \\vec v = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "We can't visualize the output vector as it's in 4 dimensions, but we _can_ look at the 4 row vectors of $A$ and the vector $\\vec v$ in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "black",
          "width": 6
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          3
         ],
         "y": [
          0,
          1
         ],
         "z": [
          0,
          4
         ]
        },
        {
         "colorscale": [
          [
           0,
           "black"
          ],
          [
           1,
           "black"
          ]
         ],
         "showlegend": false,
         "showscale": false,
         "sizemode": "raw",
         "sizeref": 0.6,
         "type": "cone",
         "u": [
          0.3
         ],
         "v": [
          0.1
         ],
         "w": [
          0.4
         ],
         "x": [
          3
         ],
         "y": [
          1
         ],
         "z": [
          4
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": [
          "row 1"
         ],
         "textfont": {
          "color": "black",
          "family": "Avenir",
          "size": 16
         },
         "textposition": "middle center",
         "type": "scatter3d",
         "x": [
          1.626491106406735
         ],
         "y": [
          0.12052668077979445
         ],
         "z": [
          2
         ]
        },
        {
         "line": {
          "color": "black",
          "width": 6
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          2
         ],
         "y": [
          0,
          1
         ],
         "z": [
          0,
          9
         ]
        },
        {
         "colorscale": [
          [
           0,
           "black"
          ],
          [
           1,
           "black"
          ]
         ],
         "showlegend": false,
         "showscale": false,
         "sizemode": "raw",
         "sizeref": 0.6,
         "type": "cone",
         "u": [
          0.2
         ],
         "v": [
          0.1
         ],
         "w": [
          0.9
         ],
         "x": [
          2
         ],
         "y": [
          1
         ],
         "z": [
          9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": [
          "row 2"
         ],
         "textfont": {
          "color": "black",
          "family": "Avenir",
          "size": 16
         },
         "textposition": "middle center",
         "type": "scatter3d",
         "x": [
          0.6095251759264189
         ],
         "y": [
          0.5
         ],
         "z": [
          4.586772183127462
         ]
        },
        {
         "line": {
          "color": "black",
          "width": 6
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          0
         ],
         "y": [
          0,
          -1
         ],
         "z": [
          0,
          0
         ]
        },
        {
         "colorscale": [
          [
           0,
           "black"
          ],
          [
           1,
           "black"
          ]
         ],
         "showlegend": false,
         "showscale": false,
         "sizemode": "raw",
         "sizeref": 0.6,
         "type": "cone",
         "u": [
          0
         ],
         "v": [
          -0.1
         ],
         "w": [
          0
         ],
         "x": [
          0
         ],
         "y": [
          -1
         ],
         "z": [
          0
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": [
          "row 3"
         ],
         "textfont": {
          "color": "black",
          "family": "Avenir",
          "size": 16
         },
         "textposition": "middle center",
         "type": "scatter3d",
         "x": [
          -0.4
         ],
         "y": [
          -0.5
         ],
         "z": [
          0
         ]
        },
        {
         "line": {
          "color": "black",
          "width": 6
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          2
         ],
         "y": [
          0,
          -2
         ],
         "z": [
          0,
          0
         ]
        },
        {
         "colorscale": [
          [
           0,
           "black"
          ],
          [
           1,
           "black"
          ]
         ],
         "showlegend": false,
         "showscale": false,
         "sizemode": "raw",
         "sizeref": 0.6,
         "type": "cone",
         "u": [
          0.2
         ],
         "v": [
          -0.2
         ],
         "w": [
          0
         ],
         "x": [
          2
         ],
         "y": [
          -2
         ],
         "z": [
          0
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": [
          "row 4"
         ],
         "textfont": {
          "color": "black",
          "family": "Avenir",
          "size": 16
         },
         "textposition": "middle center",
         "type": "scatter3d",
         "x": [
          0.7171572875253809
         ],
         "y": [
          -1.2828427124746191
         ],
         "z": [
          0
         ]
        },
        {
         "line": {
          "color": "purple",
          "width": 6
         },
         "mode": "lines",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          0
         ],
         "z": [
          0,
          3
         ]
        },
        {
         "colorscale": [
          [
           0,
           "purple"
          ],
          [
           1,
           "purple"
          ]
         ],
         "showlegend": false,
         "showscale": false,
         "sizemode": "raw",
         "sizeref": 0.6,
         "type": "cone",
         "u": [
          0.1
         ],
         "v": [
          0
         ],
         "w": [
          0.3
         ],
         "x": [
          1
         ],
         "y": [
          0
         ],
         "z": [
          3
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": [
          "v"
         ],
         "textfont": {
          "color": "purple",
          "family": "Avenir",
          "size": 16
         },
         "textposition": "middle center",
         "type": "scatter3d",
         "x": [
          0.12052668077979445
         ],
         "y": [
          0
         ],
         "z": [
          1.626491106406735
         ]
        }
       ],
       "layout": {
        "font": {
         "color": "black",
         "family": "Avenir",
         "size": 14
        },
        "height": 500,
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "scene": {
         "aspectratio": {
          "x": 1.2,
          "y": 1.2,
          "z": 0.8
         },
         "bgcolor": "white",
         "camera": {
          "eye": {
           "x": 1.25,
           "y": 2,
           "z": 0.25
          }
         },
         "xaxis": {
          "backgroundcolor": "white",
          "gridcolor": "#f0f0f0",
          "range": [
           -1,
           4
          ],
          "showbackground": true,
          "title": {
           "text": "x"
          },
          "zerolinecolor": "gray"
         },
         "yaxis": {
          "backgroundcolor": "white",
          "gridcolor": "#f0f0f0",
          "range": [
           -3,
           2
          ],
          "showbackground": true,
          "title": {
           "text": "y"
          },
          "zerolinecolor": "gray"
         },
         "zaxis": {
          "backgroundcolor": "white",
          "gridcolor": "#f0f0f0",
          "range": [
           -1,
           10
          ],
          "showbackground": true,
          "title": {
           "text": "z"
          },
          "zerolinecolor": "gray"
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import plot_vectors\n",
    "\n",
    "plot_vectors([((3, 1, 4), 'black', 'row 1'), \n",
    "              ((2, 1, 9), 'black', 'row 2'), \n",
    "              ((0, -1, 0), 'black', 'row 3'),\n",
    "              ((2, -2, 0), 'black', 'row 4'),\n",
    "              ((1, 0, 3), 'purple', 'v')],\n",
    "              vdeltax=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you'll notice that $\\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$ (in $\\color{purple} \\text{purple}$) looks the most similar to the second row vector, $\\begin{bmatrix} 2 \\\\ 1 \\\\ 9 \\end{bmatrix}$. This explains why in the product $A \\vec v = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2 \\end{bmatrix}$, the largest component ($29$) corresponds to the second row vector. $\\vec v$ and the third row vector, $\\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix}$, are orthogonal (rotate the plot so that you can see this), so the third entry in $A \\vec v$ is 0.\n",
    "\n",
    ":::{tip} Activity 2\n",
    ":class: dropdown\n",
    "\n",
    "We'll try not to bore you with mundane calculations in the future, but it's important to perform matrix-vector multiplication by hand a few times to understand how it works. \n",
    "\n",
    "In each part, perform the matrix-vector multiplication by hand or state that it cannot be done.\n",
    "\n",
    "1. $$\\begin{bmatrix} 7 & 8 & -1 \\\\ 0 & 1 & 2 \\\\ 9 & 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 4 \\\\ 8 \\end{bmatrix}$$\n",
    "1. $$\\begin{bmatrix} 7 & 8 & -1 \\\\ 0 & 1 & 2 \\\\ \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix}$$\n",
    "1. $$\\begin{bmatrix} 4 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 5 & 4 & 3 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 4 \\\\ 8 \\end{bmatrix}$$\n",
    "1. $$\\begin{bmatrix} 4 & 2 & 3 \\\\ 1 & 0 & 1 \\\\ 5 & 4 & 3 \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 7 & 8 & -1 \\\\ 0 & 1 & 2 \\\\ 9 & 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 4 \\\\ 8 \\end{bmatrix}$$\n",
    "    (While we haven't yet looked at how to compute the product of two matrices, you can still answer this just using what you know about matrix-vector multiplication.)\n",
    "\n",
    "In the cell below, use `numpy` to verify your answers. You'll need to define the matrices and vectors as `numpy` arrays, and use the `@` operator to perform the matrix-vector multiplication.\n",
    "\n",
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[7, 8, -1], [0, 1, 2], [9, 0, 1/2]])&code=B = np.array([[5], [4], [8]])&code=A @ B\"\n",
    "  width=\"100%\"\n",
    "  height=\"600px\"\n",
    "></iframe>\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Linear Combination Interpretation\n",
    "\n",
    "We've described matrix-vector multiplication as the result of taking the dot product of each row of $A$ with $\\vec v$, and indeed this is the easiest way to actually compute the output. But, there's another more important interpretation. In the above dot products, you may have noticed:\n",
    "\n",
    "- Entries in the first column of $A$ ($3$, $2$, $0$, and $2$) were always multiplied by the first element of $\\vec v$ ($1$).\n",
    "- Entries in the second column of $A$ ($1$, $1$, $-1$, and $-2$) were always multiplied by the second element of $\\vec v$ ($0$).\n",
    "- Entries in the third column of $A$ ($4$, $9$, $0$, and $0$) were always multiplied by the third element of $\\vec v$ ($3$).\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$A \\vec v = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} \\color{orange} \\mathbf{1} \\\\ \\color{orange} \\mathbf{0} \\\\ \\color{orange} \\mathbf{3} \\end{bmatrix} = \\underbrace{{\\color{orange} \\mathbf{1}} \\begin{bmatrix} 3 \\\\ 2 \\\\ 0 \\\\ 2 \\end{bmatrix} + {\\color{orange} \\mathbf{0}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -2 \\end{bmatrix} + {\\color{orange} \\mathbf{3}} \\begin{bmatrix} 4 \\\\ 9 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{\\color{orange} \\text{linear combination of columns of A}} = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "At the start of this section, we defined $A$ by stacking the vectors $\\vec u_1$, $\\vec u_2$, and $\\vec u_3$ side-by-side, and I told you to think of a matrix as a collection of column vectors. The above result is precisely why – it's because when we multiply $A$ by $\\vec v$, we're computing a linear combination of the columns of $A$, where the weights are the components of $\\vec v$!\n",
    "\n",
    "Since $A \\vec v$ produces a linear combination of the columns of $A$, a natural question to ask at this point is whether the columns of $A$ are all linearly independent. $A$ only has 3 columns, each of which is in $\\mathbb{R}^4$, so while they may or may not be linearly independent (are they?), we know they cannot span all of $\\mathbb{R}^4$, as we'd need at least 4 vectors to reach every element in $\\mathbb{R}^4$.\n",
    "\n",
    "This is the type of thinking we'll return to in Chapter 2.5. This will lead us to define the **rank** of a matrix, perhaps the single most important number associated with a matrix. That's a discussion for another day.\n",
    "\n",
    ":::{attention} The Two Pictures\n",
    "\n",
    "Any time you see a matrix-vector product, like $A \\vec v$, you should think of it not as a random operation, but as:\n",
    "\n",
    "1. (**More Important**) A linear combination of the columns of $A$, where the weights are the components of $\\vec v$.\n",
    "$$A \\vec v = v_1 \\left( \\text{column } 1 \\text{ of } A \\right) + v_2 \\left( \\text{column } 2 \\text{ of } A \\right) + \\ldots + v_d \\left( \\text{column } d \\text{ of } A \\right)$$\n",
    "\n",
    "2. A dot product of the rows of $A$ with $\\vec v$.\n",
    "$$A \\vec v = \\begin{bmatrix} (\\text{row 1 of } A) \\cdot \\vec v \\\\ (\\text{row 2 of } A) \\cdot \\vec v \\\\ \\vdots \\\\ (\\text{row n of } A) \\cdot \\vec v \\end{bmatrix}$$\n",
    ":::\n",
    "\n",
    ":::{tip} Activity 3\n",
    ":class: dropdown\n",
    "\n",
    "Consider the matrix $M$ defined below.\n",
    "\n",
    "$$M = \\begin{bmatrix} 2 & -1 & 3 & 0 & 4 \\\\ 1 & 5 & -2 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "In each of the following parts, write out $\\vec u$ concretely, compute $M \\vec u$, and explain the result in relation to the linear combination interpretation of matrix-vector multiplication.\n",
    "\n",
    "1. A vector whose second component is 1, and whose other components are 0.\n",
    "2. A vector containing all 1s.\n",
    "3. A vector containing all $\\frac{1}{5}$s.\n",
    "4. A vector whose components sum to 1, whose first component is $\\frac{3}{5}$, and whose other components are all equal to one another.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "Matrix-matrix multiplication – or just \"matrix multiplication\" – is a generalization of matrix-vector multiplication. Let's present matrix multiplication in its most general terms.\n",
    "\n",
    "### Definition\n",
    "\n",
    ":::{note} Definition: Matrix Multiplication\n",
    "Suppose:\n",
    "- $A$ is a $n \\times d$ matrix.\n",
    "- $B$ is a $d \\times p$ matrix.\n",
    "\n",
    "Then, $AB$ is a $n \\times p$ matrix such that \n",
    "- the element in row $i$ and column $j$ of $AB$ is \n",
    "- the **dot product of row $i$ of $A$ and column $j$ of $B$**, for $i = 1, 2, ..., n$ and $j = 1, 2, ..., p$.\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$(AB)_{ij} = \\left( \\text{row } i \\text{ of } A \\right) \\cdot \\left( \\text{column } j \\text{ of } B \\right) = \\sum_{k=1}^d A_{ik} B_{kj}$$\n",
    ":::\n",
    "\n",
    "Note that if $p = 1$, this reduces to the matrix-vector multiplication case from before. In that case, the only possible value of $j$ is 1, since the output only has 1 column, and the element in row $i$ of the output vector is the dot product of row $i$ in $A$ and the vector $B$ (which we earlier referred to as $\\vec v$ in the less general case).\n",
    "\n",
    "For a concrete example, suppose $A$ and $B$ are defined below:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\quad B = \\begin{bmatrix} 1 & 2\\\\ 0 & 7\\\\ 3 & 2 \\end{bmatrix}$$\n",
    "\n",
    "The number of columns of $A$ must equal the number of rows of $B$ in order for the product $AB$ to be defined, as the Golden Rule tells us. That is fortunately the case here. Since $A$ has shape $\\boxed{4} \\times 3$ and $B$ has shape $3 \\times \\boxed{2}$, the output matrix will have shape $4 \\times 2$. Each of those $4 \\cdot 2 = 8$ elements will be the dot product of a row in $A$ with a column in $B$.\n",
    "\n",
    "Here is the product of $A$ and $B$:\n",
    "\n",
    "$$AB = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ 0 & -1 & 0 \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 2\\\\ 0 & 7\\\\ 3 & 2 \\end{bmatrix} = \\begin{bmatrix} 15 & 21 \\\\ 29 & 29 \\\\ 0 & -7 \\\\ 2 & -10 \\end{bmatrix}$$\n",
    "\n",
    "Let's see if we can audit where these numbers came from. Let's consider $(AB)_{32}$, which is the element in row 3 and column 2 of the output. It should have come from the dot product of row 3 of $A$ and column 2 of $B$.\n",
    "\n",
    "$$AB = \\begin{bmatrix} 3 & 1 & 4 \\\\ 2 & 1 & 9 \\\\ \\color{#3d81f6} \\mathbf{0} & \\color{#3d81f6} \\mathbf{-1} & \\color{#3d81f6} \\mathbf{0} \\\\ 2 & -2 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & \\color{#3d81f6} \\mathbf{2}\\\\ 0 & \\color{#3d81f6} \\mathbf{7}\\\\ 3 & \\color{#3d81f6} \\mathbf{2} \\end{bmatrix} = \\begin{bmatrix} 15 & 21 \\\\ 29 & 29 \\\\ 0 & \\boxed{\\color{#3d81f6} \\mathbf{-7}} \\\\ 2 & -10 \\end{bmatrix}$$\n",
    "\n",
    "And indeed, $-7$ is the dot product of $\\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix}$ and $\\begin{bmatrix} 2 \\\\ 7 \\\\ 2 \\end{bmatrix}$, as $0 \\cdot 2 + (-1) \\cdot 7 + 0 \\cdot 2 = -7$.\n",
    "\n",
    "You should notice that many of the numbers in the output $AB$ look familiar. That's because we used the same $A$ as we did earlier in the section, and the first column of $B$ is just $\\vec v$ from the matrix-vector example. So, the first column in $AB$ is the same as the vector $A \\vec v = \\begin{bmatrix} 15 \\\\ 29 \\\\ 0 \\\\ 2\\end{bmatrix}$ as we computed earlier. The difference now is that the output $AB$ isn't just a single vector, but is a matrix with 2 columns. The second column, $\\begin{bmatrix} 21 \\\\ 29 \\\\ -7 \\\\ -10\\end{bmatrix}$, comes from multiplying $A$ by the second column in $B$, $\\begin{bmatrix} 2 \\\\ 7 \\\\ 2\\end{bmatrix}$.\n",
    "\n",
    "Note that as we add columns to $B$, we'd add columns to the output. If $B$ had 10 columns, then $AB$ would have 10 columns, too, without $A$ needing to change. As long as the Golden Rule – that the number of columns in $A$ equals the number of rows in $B$ – holds, the product $AB$ can be computed, and it has shape $(\\text{number of rows in } A) \\times (\\text{number of columns in } B)$.\n",
    "\n",
    "### Properties\n",
    "\n",
    ":::{note} Properties of Matrix Multiplication\n",
    "\n",
    "- Matrix multiplication is **associative**. That is, $(AB)C = A(BC)$ for any matrices $A$, $B$, and $C$ that are of the appropriate shapes.\n",
    "- Matrix multiplication is **distributive**. That is, $A(B + C) = AB + AC$ for any matrices $A$, $B$, and $C$ that are of the appropriate shapes.\n",
    "- Matrix multiplication is **not commutative**! That is, in general, $AB \\neq BA$ for any matrices $A$ and $B$ that are of the appropriate shapes.\n",
    "\n",
    ":::\n",
    "\n",
    "The first two properties – associativity and distributivity – match standard arithmetic properties that we've become accustomed to. The associative property allows you to, for example, compute $AB \\vec v$ only by using matrix-vector multiplications, since you can first multiply $B \\vec v$, which results in a vector, and then multiply $A$ by that vector. (I had you do this in Activity 2 earlier in this section – I hope you did it! 🧐)\n",
    "\n",
    "The fact that matrix multiplication is **not** commutative may come as a surprise, as every other form of multiplication you've learned about up until this point has been commutative (including the dot product).\n",
    "\n",
    ":::{warning} **In general, $AB \\neq BA$.**\n",
    ":::\n",
    "\n",
    "In fact, if $AB$ exists, $BA$ may or may not! If $A$ is $n \\times d$ and $B$ is $d \\times p$, then $BA$ only exists if $n = p$. But even then, $AB \\neq BA$ in general.\n",
    "\n",
    "For example, if $A$ is $2 \\times 3$ and $B$ is $3 \\times 2$, then $AB$ is $2 \\times 2$ and $BA$ is $3 \\times 3$; here, both products exist, but they cannot be equal since they have different shapes.\n",
    "\n",
    "Even if $A$ and $B$ are both $n \\times n$, $AB \\neq BA$ in general. For illustration, consider: \n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}$$\n",
    "\n",
    "Then, $$AB = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix} \\neq BA = \\begin{bmatrix} 23 & 34 \\\\ 31 & 46 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip} Activity 4\n",
    ":class: dropdown\n",
    "\n",
    "**Activity 4.1**\n",
    "\n",
    "Let $P = \\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$, $S = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}$, and $\\vec v = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}$.\n",
    "\n",
    "1. Evaluate $P \\vec v$ and $S \\vec v$. Then, explain in words what multiplying $P$ and $S$ by $\\vec v$ does to $\\vec v$.\n",
    "1. Evaluate $PS \\vec v$ and $SP \\vec v$. The results should be different, as we'd expect, since matrix multiplication is not commutative in general. Explain the difference intuitively, given the \"operations\" $P$ and $S$ perform on $\\vec v$.\n",
    "\n",
    "$P$ is called a permutation matrix, and $S$ is called a diagonal matrix.\n",
    "\n",
    "**Activity 4.2**\n",
    "\n",
    "The famous Fibonacci sequence of integers, $F_0, F_1, F_2, \\ldots$, is defined as follows:\n",
    "\n",
    "$$F_0 = 0, \\quad F_1 = 1, \\quad F_n = F_{n-1} + F_{n-2} \\text{ for } n \\geq 2$$\n",
    "\n",
    "The first few terms in the sequence are $0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, \\ldots$.\n",
    "\n",
    "It turns out you can compute the $n$th term in the sequence using matrix multiplication. \n",
    "\n",
    "1. Find a $2 \\times 2$ matrix $A$ such that $A \\begin{bmatrix} F_{n-1} \\\\ F_{n-2} \\end{bmatrix} = \\begin{bmatrix} F_n \\\\ F_{n-1} \\end{bmatrix}$. The answer uses relatively small numbers.\n",
    "1. Compute $A \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, i.e. the product of $A$ and the vector $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n",
    "1. Since $A$ is square, we can multiply it by itself. Compute $A^2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $A^3 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n",
    "\n",
    "If you continue this process, you'll find that $A^n \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ is a vector containing the $n$th and $(n-1)$th terms in the Fibonacci sequence!\n",
    "\n",
    "**Activity 4.3**\n",
    "\n",
    "Using the same matrices $P$ and $Q$ from Activity 4.1, compute $(P - Q) \\vec v$ and $P\\vec v - Q \\vec v$. Are both the results the same? If so, what property of matrix multiplication guarantees this?\n",
    "\n",
    "Is the result of $(P - Q) \\vec v$ interpretable, in the same way that the results of $P \\vec v$ and $Q \\vec v$ were in Activity 4.1?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "I've shown you the naïve – and by far most common – algorithm for matrix multiplication. If $A$ and $B$ are both square $n \\times n$ matrices, then the runtime of the naïve algorithm is $O(n^3)$.\n",
    "\n",
    "However, there exist more efficient algorithms for matrix multiplication. Strassen's algorithm is one such example; it describes how to multiply two square $n \\times n$ matrices in $O(n^{2.807})$ time. The study of efficient algorithms for matrix multiplication is an active area of research; if you're interested in learning more, look [here](https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication).\n",
    "\n",
    "Matrix multiplication, I might argue, is one of the reasons NVIDIA is the most valuable company in the world. Modern machine learning is built on matrix multiplication, and GPUs are optimized for it. Why? This comment [from Reddit](https://www.reddit.com/r/explainlikeimfive/comments/zpso6w/eli5_what_about_gpu_architecture_makes_them/) does a good job of explaining:\n",
    "\n",
    "> Imagine you have 1 million math assignments to do, they are very simple assignments, but there are a lot that need to be done, they are not dependent on each other so they can be done on any order.\n",
    "> \n",
    "> You have two options, distribute them to 10 thousand people to do it in parallel or give them to 10 math experts. The experts are very fast, but hey, there are only 10 of them, the 10 thousand are more suitable for the task because they have the \"brute force\" for this.\n",
    ">\n",
    "> GPUs have thousands of cores, CPUs have tens.\n",
    "\n",
    "On that note, the `@` operator in `numpy` is used for matrix multiplication; it is a shorthand for `np.matmul`. You can also use it to multiply a matrix by a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=B = np.array([[1, 2], [0, 7], [3, 2]])&code=A&code=B&code=A @ B # Shorthand for np.matmul(A, B)\"\n",
    "  width=\"100%\"\n",
    "  height=\"500px\"\n",
    "></iframe>\n",
    "\n",
    ":::{warning} The `*` Operator is Not Matrix Multiplication!\n",
    "\n",
    "The `*` operator in `numpy` is used for element-wise multiplication, not matrix multiplication. If you want to multiply two matrices, you must use the `@` operator.\n",
    "\n",
    "In the cell above, try and compute the product `A * B` using the `*` operator. What happens?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Transpose\n",
    "\n",
    "There's an important operation on matrices that we haven't discussed yet.\n",
    "\n",
    "### Definition\n",
    "\n",
    ":::{note} Definition: Transpose\n",
    "\n",
    "The **transpose** of a matrix $A$ is the matrix $A^T$ with entries $A^T_{ij} = A_{ji}$ for all $i, j$.\n",
    "\n",
    "Intuitively, the transpose results from replacing the rows of $A$ with the columns of $A$ and vice-versa.\n",
    ":::\n",
    "\n",
    "To illustrate, let's start with our familiar matrix $A$:\n",
    "\n",
    "$$A = \\begin{bmatrix} 3 & {\\color{#3d81f6} \\mathbf{1}} & 4 \\\\ 2 & {\\color{#3d81f6} \\mathbf{1}} & 9 \\\\ 0 & {\\color{#3d81f6} \\mathbf{-1}} & 0 \\\\ 2 & {\\color{#3d81f6} \\mathbf{-2}} & 0 \\end{bmatrix}$$\n",
    "\n",
    "The transpose of $A$ is:\n",
    "\n",
    "$$A^T = \\begin{bmatrix} 3 & 2 & 0 & 2 \\\\ {\\color{#3d81f6} \\mathbf{1}} & {\\color{#3d81f6} \\mathbf{1}} & {\\color{#3d81f6} \\mathbf{-1}} & {\\color{#3d81f6} \\mathbf{-2}} \\\\ 4 & 9 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Note that $A \\in \\mathbb{R}^{4 \\times 3}$ and $A^T \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "Why would we ever need to do this? To illustrate, suppose $\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\end{bmatrix}$, and that we'd like to compute the product $A^T \\vec u$. (Note that $\\vec u$ must be in $\\mathbb{R}^4$ in order for $A^T \\vec u$ to be defined, unlike $\\vec v \\in \\mathbb{R}^3$ in the product $A \\vec v$). Then:\n",
    "\n",
    "$$\\begin{align*}A^T \\vec u &= \\begin{bmatrix} 3 & 2 & 0 & 2 \\\\ 1 & 1 & -1 & -2 \\\\ 4 & 9 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} {\\color{orange} \\mathbf{u_1}} \\\\ {\\color{orange} \\mathbf{u_2}} \\\\ {\\color{orange} \\mathbf{u_3}} \\\\ {\\color{orange} \\mathbf{u_4}} \\end{bmatrix} \\\\ &= {\\color{orange} \\mathbf{u_1}} \\begin{bmatrix} 3 \\\\ 1 \\\\ 4 \\end{bmatrix} + {\\color{orange} \\mathbf{u_2}} \\begin{bmatrix} 2 \\\\ 1 \\\\ 9 \\end{bmatrix} + {\\color{orange} \\mathbf{u_3}} \\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix} + {\\color{orange} \\mathbf{u_4}} \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\end{bmatrix} \\end{align*}$$\n",
    "\n",
    "This is a linear combination of the **rows** of $A$, where the weights are the components of $\\vec u$. Remember, the standard product $A \\vec v$ is a linear combination of the columns of $A$, so the transpose helps us if we want to compute a linear combination of the rows of $A$. (Equivalently, it helps us if we want to compute the dot product of the **columns of $A$** with $\\vec u$ – see the \"Two Pictures\" note from earlier in this chapter.)\n",
    "\n",
    "The transpose also gives us another way of expressing the dot product of two vectors. If $\\vec u$ and $\\vec v$ are two vectors in $\\mathbb{R}^n$, then $\\vec u^T$ is a row vector with 1 row and $n$ columns. Multiplying $\\vec u^T$ by $\\vec v$ results in a $1 \\times 1$ matrix, which is just the scalar $\\vec u \\cdot \\vec v$.\n",
    "\n",
    "$$\n",
    "\\vec {\\color{#3d81f6}u}^T \\vec{\\color{purple}v} = \\begin{bmatrix} {\\color{#3d81f6}u_1} & {\\color{#3d81f6}u_2} & \\ldots & {\\color{#3d81f6}u_n} \\end{bmatrix} \\begin{bmatrix}{\\color{purple}v_1} \\\\{\\color{purple}v_2} \\\\ \\vdots \\\\{\\color{purple}v_n} \\end{bmatrix} = {\\color{#3d81f6}u_1}{\\color{purple}v_1} + {\\color{#3d81f6}u_2}{\\color{purple}v_2} + \\ldots + {\\color{#3d81f6}u_n}{\\color{purple}v_n} = \\vec {\\color{#3d81f6}u} \\cdot \\vec{\\color{purple}v} = \\vec{\\color{purple}v} \\cdot \\vec {\\color{#3d81f6}u} = \\vec{\\color{purple}v}^T \\vec {\\color{#3d81f6}u}\n",
    "$$\n",
    "\n",
    ":::{warning} Transpose, or Dot, but Not Both!\n",
    "\n",
    "The following are all valid ways of computing the dot product of $\\vec u$ and $\\vec v$, assuming they have the same number of components:\n",
    "\n",
    "$$\n",
    "\\vec u \\cdot \\vec v = \\vec u^T \\vec v = \\vec v^T \\vec u = \\vec v \\cdot \\vec u\n",
    "$$\n",
    "\n",
    "However, $\\vec u^T \\cdot \\vec v$ **is not defined**. The dot product is only defined for two vectors of the same dimensions, but $\\vec u^T$ is a row vector with $n$ columns, and $\\vec v$ is a column vector with $n$ rows.\n",
    "\n",
    ":::\n",
    "\n",
    "The benefit of using the transpose to express the dot product is that it allows us to write the dot product of two vectors in terms of matrix multiplication, rather than being an entirely different type of operation. (In fact, as we've seen here, matrix multiplication is just a generalization of the dot product.)\n",
    "\n",
    "There are other uses for the transpose, too, so it's a useful tool to have in your toolbox.\n",
    "\n",
    "### Properties\n",
    "\n",
    ":::{note} Properties of the Transpose\n",
    "- $(A^T)^T = A$, i.e. the transpose of the transpose of a matrix is the original matrix.\n",
    "- $(A + B)^T = A^T + B^T$.\n",
    "- $(cA)^T = cA^T$ for any scalar $c \\in \\mathbb{R}$.\n",
    "- $(AB)^T = B^T A^T$.\n",
    ":::\n",
    "\n",
    "The first three properties are relatively straightforward. The last property is a bit more subtle. Try and reason as to why it's true on your own, then peek into the box below to verify your reasoning and to see an example.\n",
    "\n",
    ":::{admonition} Why is $(AB)^T = B^T A^T$?\n",
    ":class: dropdown\n",
    "\n",
    "Let's start with just $(AB)^T$ and reason our way from there. Define $C = (AB)^T$. $C$ is presumably the product of two matrices $X$ and $Y$, we just don't know what $X$ and $Y$ are. By the definition of matrix multiplication, we know that $C_{ij}$ is the dot product of the $i$th row of $X$ and the $j$th column of $Y$. How can we express the product $C = XY$ in terms of $A$ and $B$?\n",
    "\n",
    "Let's work backwards. Since $C_{ij} = (AB)^T_{ij} = (AB)_{ji}$ by the definition of the transpose, we know that: \n",
    "\n",
    "$$C_{ij} = (AB)_{ji} = \\left( \\text{row } j \\text{ of } A \\right) \\cdot \\left( \\text{column } i \\text{ of } B \\right) = \\left( \\text{column } i \\text{ of } B \\right) \\cdot \\left( \\text{row } j \\text{ of } A \\right)$$\n",
    "\n",
    "This is a little backwards relative to the definition of matrix multiplication, which says that:\n",
    "\n",
    "$$C_{ij} = (XY)_{ij} = \\left( \\text{row } i \\text{ of } X \\right) \\cdot \\left( \\text{column } j \\text{ of } Y \\right)$$\n",
    "\n",
    "In order for the two definitions of $C_{ij}$ to be consistent, we must have:\n",
    "\n",
    "$$\\left( \\text{column } i \\text{ of } B \\right) \\cdot \\left( \\text{row } j \\text{ of } A \\right) = \\left( \\text{row } i \\text{ of } X \\right) \\cdot \\left( \\text{column } j \\text{ of } Y \\right)$$\n",
    "\n",
    "- Row $i$ of $X$ is the same as column $i$ of $B$, if $X = B^T$.\n",
    "- Column $j$ of $Y$ is the same as row $j$ of $A$, if $Y = A^T$.\n",
    "\n",
    "Putting this together, we have:\n",
    "\n",
    "$$C = (AB)^T = B^T A^T$$\n",
    "\n",
    "as we hoped!\n",
    "\n",
    "To make things concrete, let's consider two new matrices $A$ and $B$:\n",
    "\n",
    "$$A = \\begin{bmatrix} 0 & -1 \\\\ 4 & 2 \\\\ 3 & 9 \\\\ 0 & 1 \\end{bmatrix} \\quad B = \\begin{bmatrix} 1 & 2 & 3 \\\\ -1 & -2 & 4 \\end{bmatrix}$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$AB = \\begin{bmatrix} 0 & -1 \\\\ 4 & 2 \\\\ {\\color{#3d81f6} \\mathbf{3}} & {\\color{#3d81f6} \\mathbf{9}} \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & {\\color{#3d81f6} \\mathbf{3}} \\\\ -1 & -2 & {\\color{#3d81f6} \\mathbf{4}} \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & -4 \\\\ 2 & 4 & 20 \\\\ -6 & -12 & \\boxed{{\\color{#3d81f6} \\mathbf{45}}} \\\\ -1 & -2 & 4 \\end{bmatrix}$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$B^T A^T = \\begin{bmatrix} 1 & -1 \\\\ 2 & -2 \\\\ {\\color{#3d81f6} \\mathbf{3}} & {\\color{#3d81f6} \\mathbf{4}} \\end{bmatrix} \\begin{bmatrix} 0 & 4 & {\\color{#3d81f6} \\mathbf{3}} & 0 \\\\ -1 & 2 & {\\color{#3d81f6} \\mathbf{9}} & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & -6 & -1 \\\\ 2 & 4 & -12 & -2 \\\\ -4 & 20 & \\boxed{{\\color{#3d81f6} \\mathbf{45}}} & 4 \\end{bmatrix}$$\n",
    "\n",
    "$B^T A^T$ is the transpose of $AB$. Both $AB$ and $B^T A^T$ have 12 elements, both computed using the same 12 dot products.\n",
    "\n",
    ":::\n",
    "\n",
    "The fact that $(AB)^T = B^T A^T$ comes in handy when finding the norm of a matrix-vector product. If $A$ is an $n \\times d$ matrix and $\\vec v \\in \\mathbb{R}^d$, then:\n",
    "\n",
    "$$\\lVert A \\vec v \\rVert^2 = (A \\vec v)^T (A \\vec v) = \\vec v^T A^T A \\vec v$$\n",
    "\n",
    "As we'll soon see, some matrices $A$ have special properties that make this computation particularly easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy`, the `T` attribute is used to compute the transpose of a 2D array.\n",
    "\n",
    "<iframe\n",
    "  src=\"https://jupyterlite.github.io/demo/repl/index.html?kernel=python&code=import numpy as np&code=A = np.array([[3, 1, 4], [2, 1, 9], [0, -1, 0], [2, -2, 0]])&code=A&code=A.T\"\n",
    "  width=\"100%\"\n",
    "  height=\"175%\"\n",
    "></iframe>\n",
    "\n",
    ":::{tip} Activity 6\n",
    ":class: dropdown\n",
    "\n",
    "**Activity 6.1**\n",
    "\n",
    "In the cell above:\n",
    "\n",
    "1. Define `v` to be an array corresponding to the vector $\\vec v = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$.\n",
    "2. Find the norm of the product $A \\vec v$ using `np.linalg.norm`.\n",
    "3. Find the norm of the product $A \\vec v$ using the fact that $\\lVert A \\vec v \\rVert^2 = \\vec v^T A^T A \\vec v$, and verify that you get the same answer.\n",
    "\n",
    "**Activity 6.2**\n",
    "\n",
    "Suppose $M \\in \\mathbb{R}^{n \\times d}$ is a matrix, $\\vec{x} \\in \\mathbb{R}^d$ is a vector, and $s \\in \\mathbb{R}$ is a scalar.\n",
    "\n",
    "Determine whether each of the following quantities is a matrix, vector, scalar, or undefined. If the result is a matrix or vector, determine its dimensions.\n",
    "\n",
    "1. $M\\vec{x}$\n",
    "\n",
    "2. $\\vec{x} M$\n",
    "\n",
    "3. $\\vec{x}^2$\n",
    "\n",
    "4. $M^TM$\n",
    "\n",
    "5. $MM^T$\n",
    "\n",
    "6. $\\vec{x}^T M \\vec{x}$\n",
    "\n",
    "7. $(sM\\vec{x}) \\cdot (sM\\vec{x})$\n",
    "\n",
    "8. $(s \\vec{x}^T M^T)^T$\n",
    "\n",
    "9. $\\vec{x}^T M^T M \\vec{x}$\n",
    "\n",
    "10. $\\vec{x}\\vec{x}^T + M^TM$\n",
    "\n",
    "11. $\\frac{M \\vec{x}}{\\lVert \\vec{x} \\rVert} + (\\vec{x}^T M^T M \\vec{x}) M \\vec{x}$\n",
    "\n",
    "**Activity 6.3**\n",
    "\n",
    "Let $A = \\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\\\ -1 & 1 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 & 0 & 2 \\\\ 2 & 1 & 3 \\end{bmatrix}$, and $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ -1 & -1 \\end{bmatrix}$.\n",
    "\n",
    "1. Compute $AB$, then multiply the result by $C$.\n",
    "1. Compute $A$, then multiply the result by $BC$. Do you get the same result as above? If so, what property of matrix multiplication guarantees this?\n",
    "1. Determine a formula for $(ABC)^T$, and verify that your result works. (Hint: Start with the fact that $(AB)^T = B^T A^T$.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Identity Matrix\n",
    "\n",
    ":::{admonition} Definition: Identity Matrix\n",
    "\n",
    "The **identity matrix** is the square matrix $I$ with ones on the diagonal and zeros everywhere else.\n",
    "\n",
    "$$I = \\begin{bmatrix} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}$$\n",
    "\n",
    ":::\n",
    "\n",
    "Saying \"the identity matrix\" is a bit ambiguous, as there are infinitely many identity matrices – there's a $1 \\times 1$ identity matrix, a $2 \\times 2$ identity matrix, a $3 \\times 3$ identity matrix, and so on. Often, the dimension of the identity matrix is implied by context, and if not, we might provide it as a subscript, e.g. $I_n$ for the $n \\times n$ identity matrix.\n",
    "\n",
    "Why is the identity matrix defined this way? It's the matrix equivalent of the number $1$ in scalar multiplication, also known as the multiplicative identity. If $c$ is a scalar, then $c \\cdot 1 = c$ and $1 \\cdot c = c$. (0 is known as the additive identity in scalar multiplication.)\n",
    "\n",
    "Similarly, if $A$ is **square** $n \\times n$ matrix and $\\vec v \\in \\mathbb{R}^n$ is a vector, then the $n \\times n$ identity matrix $I$ is the unique matrix that satisfies:\n",
    "- $I \\vec v = \\vec v$ for all $\\vec v \\in \\mathbb{R}^n$.\n",
    "- $I A = A I = A$ for all $A \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "A good exercise is to verify that the identity matrix satisfies these properties.\n",
    "\n",
    ":::{tip} Activity 7\n",
    ":class: dropdown\n",
    "\n",
    "Let $X = \\begin{bmatrix} 1 & -2 \\\\ -1 & 3 \\\\ 2 & 0 \\\\ 0 & -1 \\\\ 3 & 2 \\end{bmatrix}$.\n",
    "\n",
    "1. Compute $X^TX$. \n",
    "2. Then, compute the transpose of $X^TX$. What do you notice? ($X^TX$ is called a _symmetric_ matrix.)\n",
    "3. Compute $X^TX + \\frac{1}{2} I$. We'll use matrices of the form $X^TX + \\lambda I$ in Chapter 5.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
